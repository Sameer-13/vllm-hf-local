{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__8crnA-n060"
      },
      "source": [
        "# vLLM with Llama-3.2-1B-it for Reveiw Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvRAO94HoDLo"
      },
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxDLLXlZoIYs"
      },
      "outputs": [],
      "source": [
        "!hf auth login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfesN7HQg3i3"
      },
      "source": [
        "## 2. Configeration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6McWFcsRg7U9"
      },
      "outputs": [],
      "source": [
        "model = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "api_key = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nip8O2uty4Wu"
      },
      "outputs": [],
      "source": [
        "# Stop any old servers\n",
        "!pkill -f \"vllm serve\" || true\n",
        "\n",
        "# Run in background\n",
        "!nohup vllm serve $model \\\n",
        "  --host 127.0.0.1 \\\n",
        "  --port 8000 \\\n",
        "  --dtype auto \\\n",
        "  --api-key $api_key &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cl2nWrI6dc3U"
      },
      "outputs": [],
      "source": [
        "for i in range(30):\n",
        "  # quick peek at the last lines of the log (non-blocking)\n",
        "  !sleep 2; tail -n 30 nohup.out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAn7OgeGfTl7"
      },
      "source": [
        "## 3. Testing Reachability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xNPqGvqyfTYR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "waiting for vLLM…\n",
            "waiting for vLLM…\n",
            "waiting for vLLM…\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i % \u001b[32m10\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m     20\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mwaiting for vLLM…\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# if not ready:\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m#     print(\"Server not ready. Recent logs:\")\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m#     print(subprocess.run([\"bash\",\"-lc\",\"tail -n 80 nohup.out\"], capture_output=True, text=True).stdout)\u001b[39;00m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "import time, httpx, subprocess\n",
        "\n",
        "BASE_URL = \"http://127.0.0.1:8000/v1\"\n",
        "HEADERS  = {\"Authorization\": f\"Bearer {api_key}\"}\n",
        "\n",
        "ready = False\n",
        "for i in range(180):  # up to ~3 minutes on first load\n",
        "    try:\n",
        "        # 1) unauthenticated health probe\n",
        "        if httpx.get(\"http://172.20.70.56:8000/health\", timeout=1.0).status_code == 200:\n",
        "            # 2) authenticated check on /v1/models\n",
        "            r = httpx.get(f\"{BASE_URL}/models\", headers=HEADERS, timeout=2.0)\n",
        "            if r.status_code == 200:\n",
        "                print(\"vLLM is ready ✅\")\n",
        "                ready = True\n",
        "                break\n",
        "    except Exception:\n",
        "        pass\n",
        "    if i % 10 == 0:\n",
        "        print(\"waiting for vLLM…\")\n",
        "    time.sleep(1)\n",
        "\n",
        "# if not ready:\n",
        "#     print(\"Server not ready. Recent logs:\")\n",
        "#     print(subprocess.run([\"bash\",\"-lc\",\"tail -n 80 nohup.out\"], capture_output=True, text=True).stdout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUp2Og8Sop3e"
      },
      "source": [
        "## 4. Inferencing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgsJE3CFpU5v"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "review_text = \"الطعام لذيذ والشيش افضل شيش ذقته روعه ويحتاج فقط اعاده تأهيل المبنى والتوسعه\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "You are given a customer review in Arabic.\n",
        "Evaluate how satisfied the customer seems with the place, its condition, and its services.\n",
        "Output a single integer score from 1 to 10, where:\n",
        "1 = very unhappy and dissatisfied\n",
        "10 = extremely happy and satisfied\n",
        "\n",
        "ONLY output the integer, nothing else.\n",
        "\n",
        "Review: {review_text}\n",
        "\"\"\"\n",
        "\n",
        "client = OpenAI(base_url=BASE_URL, api_key=api_key)\n",
        "\n",
        "resp = client.chat.completions.create(\n",
        "    model=model,\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    max_tokens=3,\n",
        ")\n",
        "\n",
        "predicted_rating = resp.choices[0].message.content.strip()\n",
        "print(predicted_rating)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "vllm_client",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
